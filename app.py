import streamlit as st
import os
from agent import gragh_build, query_analyzer_agent, search_agent, information_evaluator_agent, information_completer_agent, response_generator_agent
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver

# ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.set_page_config(page_title="RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒª", page_icon="ğŸ¤–", layout="wide")
st.title("RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒª")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€è³ªå•ã«å¯¾ã—ã¦é–¢é€£ã™ã‚‹è¨­è¨ˆæ›¸ã‚’æ¤œç´¢ã—ã€å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ãƒãƒƒãƒè¨­è¨ˆã‚„ç”»é¢è¨­è¨ˆã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚
""")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.thread_id = "1"  # å›ºå®šã®ã‚¹ãƒ¬ãƒƒãƒ‰ID

# ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
@st.cache_resource
def get_graph():
    return gragh_build()

graph = get_graph()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
with st.sidebar:
    st.header("ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    show_raw_response = st.checkbox("è©³ç´°ãªå‡¦ç†çµæœã‚’è¡¨ç¤º", value=False)
    st.divider()
    st.subheader("ä½¿ç”¨å¯èƒ½ãªè³ªå•ä¾‹:")
    st.markdown("""
    - å—æ³¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ç®‡æ‰€ã‚’æ´—ã„å‡ºã—ã¦ãã ã•ã„
    - åœ¨åº«ç®¡ç†ã«é–¢ã™ã‚‹ç”»é¢ã®æ©Ÿèƒ½ã‚’æ•™ãˆã¦ãã ã•ã„
    - ç™ºé€ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒãƒƒãƒã®å‡¦ç†å†…å®¹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
    - å—æ³¨ç¢ºå®šãƒãƒƒãƒã¨ç™ºé€ãƒãƒƒãƒã®é€£æºã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
    """)

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
user_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

if user_input:
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # å‡¦ç†ä¸­ã®è¡¨ç¤º
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("å‡¦ç†ä¸­ã§ã™...")
        
        # çŠ¶æ…‹ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®æº–å‚™
        config = {"configurable": {"thread_id": st.session_state.thread_id}}
        
        try:
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡¦ç†çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®è¾æ›¸
            processing_results = {
                "user_query": user_input,
                "analyzed_questions": None,
                "search_results": None,
                "evaluation_results": None,
                "final_response": None
            }
            
            # è©³ç´°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã®å‡¦ç†
            if show_raw_response:
                # æ‰‹å‹•ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè¡Œ
                with st.status("è³ªå•ã®åˆ†æä¸­...", expanded=False) as status:
                    # è³ªå•åˆ†æ
                    state = {"messages": [HumanMessage(content=user_input)]}
                    analysis_result = query_analyzer_agent(state)
                    processing_results["analyzed_questions"] = analysis_result["questions"]
                    status.update(label="è³ªå•åˆ†æå®Œäº†", state="complete")
                
                with st.status("é–¢é€£æƒ…å ±ã®æ¤œç´¢ä¸­...", expanded=False) as status:
                    # æ¤œç´¢
                    state.update(analysis_result)
                    search_result = search_agent(state)
                    processing_results["search_results"] = search_result["relevant_documents"]
                    status.update(label="æ¤œç´¢å®Œäº†", state="complete")
                
                with st.status("æƒ…å ±ã®è©•ä¾¡ä¸­...", expanded=False) as status:
                    # æƒ…å ±è©•ä¾¡
                    state.update(search_result)
                    evaluation_result = information_evaluator_agent(state)
                    processing_results["evaluation_results"] = evaluation_result
                    status.update(label="è©•ä¾¡å®Œäº†", state="complete")
                
                with st.status("æƒ…å ±ã®è£œå®Œä¸­...", expanded=False) as status:
                    # æƒ…å ±è£œå®Œï¼ˆå¿…è¦ãªå ´åˆï¼‰
                    if evaluation_result.get("has_information_gap", False):
                        state.update(evaluation_result)
                        completion_result = information_completer_agent(state)
                        state.update(completion_result)
                        status.update(label="æƒ…å ±è£œå®Œå®Œäº†", state="complete")
                    else:
                        state.update(evaluation_result)
                        status.update(label="è£œå®Œä¸è¦", state="complete")
                
                with st.status("å›ç­”ã®ç”Ÿæˆä¸­...", expanded=False) as status:
                    # å›ç­”ç”Ÿæˆ
                    response_result = response_generator_agent(state)
                    final_response = response_result.get("final_response", "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    processing_results["final_response"] = final_response
                    status.update(label="å›ç­”ç”Ÿæˆå®Œäº†", state="complete")
                
                # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                st.subheader("å‡¦ç†è©³ç´°")
                
                # åˆ†æã•ã‚ŒãŸè³ªå•ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¨æ¤œç´¢ã‚¯ã‚¨ãƒªã®è¡¨ç¤º
                st.write("ã€åˆ†æã•ã‚ŒãŸè³ªå•ã€‘")
                for i, q in enumerate(processing_results["analyzed_questions"]):
                    st.write(f"è³ªå• {i+1}: ã‚«ãƒ†ã‚´ãƒª `{q.question_category}`, æ¤œç´¢ã‚¯ã‚¨ãƒª: `{q.search_query}`")
                
                # æ¤œç´¢çµæœã®è¡¨ç¤º
                st.write("ã€é–¢é€£æ–‡æ›¸ã€‘")
                for i, doc in enumerate(processing_results["search_results"][:3]):  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
                    st.write(f"æ–‡æ›¸ {i+1}: {doc['metadata'].get('source', 'ä¸æ˜')}")
                    with st.expander(f"å†…å®¹ã‚’è¡¨ç¤º"):
                        st.write(doc['content'])
                        st.write(f"ã‚¹ã‚³ã‚¢: {doc['score']}")
                
                # æœ€çµ‚å›ç­”
                st.write("ã€æœ€çµ‚å›ç­”ã€‘")
                message_placeholder.markdown(final_response)
            else:
                # ã‚°ãƒ©ãƒ•ã‚’ä½¿ã£ãŸå®Ÿè¡Œï¼ˆé€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼‰
                events = graph.stream(
                    {"messages": [HumanMessage(content=user_input)]},
                    config,
                    stream_mode="values",
                )
                
                # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦è¡¨ç¤º
                final_response = None
                for event in events:
                    if event.get("messages") and len(event["messages"]) > 0:
                        final_message = event["messages"][-1]
                        if hasattr(final_message, "content"):
                            final_response = final_message.content
                
                if final_response:
                    message_placeholder.markdown(final_response)
                else:
                    message_placeholder.markdown("å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            
            # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«å›ç­”ã‚’è¿½åŠ 
            st.session_state.messages.append({"role": "assistant", "content": final_response if final_response else "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"})
            
        except Exception as e:
            error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            message_placeholder.markdown(error_message)
            st.error(error_message)
            import traceback
            st.error(traceback.format_exc())

# ãƒ•ãƒƒã‚¿ãƒ¼
st.divider()
st.markdown("Â© 2025 RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒª")