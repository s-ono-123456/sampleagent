import streamlit as st
import os
from agent import gragh_build, query_analyzer_agent, search_agent, information_evaluator_agent, information_completer_agent, response_generator_agent
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from streamlit_mermaid import st_mermaid

# ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.set_page_config(page_title="RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒª", page_icon="ğŸ¤–", layout="wide")
st.title("RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒª")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€è³ªå•ã«å¯¾ã—ã¦é–¢é€£ã™ã‚‹è¨­è¨ˆæ›¸ã‚’æ¤œç´¢ã—ã€å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
ãƒãƒƒãƒè¨­è¨ˆã‚„ç”»é¢è¨­è¨ˆã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚
""")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.thread_id = "1"  # å›ºå®šã®ã‚¹ãƒ¬ãƒƒãƒ‰ID

# ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
@st.cache_resource
def get_graph():
    return gragh_build()

graph = get_graph()


# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
with st.sidebar:
    st.header("ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    st.divider()
    st.subheader("ä½¿ç”¨å¯èƒ½ãªè³ªå•ä¾‹:")
    st.markdown("""
    - å—æ³¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ç®‡æ‰€ã‚’æ´—ã„å‡ºã—ã¦ãã ã•ã„
    - åœ¨åº«ç®¡ç†ã«é–¢ã™ã‚‹ç”»é¢ã®æ©Ÿèƒ½ã‚’æ•™ãˆã¦ãã ã•ã„
    - ç™ºé€ãƒ©ãƒ™ãƒ«ç”Ÿæˆãƒãƒƒãƒã®å‡¦ç†å†…å®¹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„
    - å—æ³¨ç¢ºå®šãƒãƒƒãƒã¨ç™ºé€ãƒãƒƒãƒã®é€£æºã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„
    """)
    
    # ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆè¡¨ç¤ºç”¨ã®ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
    if st.button("ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º"):
        st.session_state.show_flowchart = True

# ã‚¿ãƒ–ã®ä½œæˆ
tab1, tab2 = st.tabs(["å›ç­”", "ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ"])

with tab1:
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    user_input = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")

    if user_input:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # å‡¦ç†ä¸­ã®è¡¨ç¤º
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("å‡¦ç†ä¸­ã§ã™...")
            
            # çŠ¶æ…‹ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®æº–å‚™
            config = {"configurable": {"thread_id": st.session_state.thread_id}}
            
            try:
                # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡¦ç†çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®è¾æ›¸
                processing_results = {
                    "user_query": user_input,
                    "analyzed_questions": None,
                    "search_results": None,
                    "evaluation_results": None,
                    "final_response": None
                }
                
                # è©³ç´°è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã®å‡¦ç†
                # Graphã‚’ä½¿ç”¨ã—ã¤ã¤ã€è©³ç´°æƒ…å ±ã‚’åé›†
                processing_results = {
                    "user_query": user_input,
                    "analyzed_questions": [],
                    "search_results": [],
                    "final_response": None
                }
                endflg = False
                with st.status("å‡¦ç†ä¸­...", expanded=True) as status:
                    for chunk in graph.stream(
                        {"messages": [HumanMessage(content=user_input)],
                            "last_node": ""},
                        config,
                        stream_mode="values",
                    ):
                        # ã‚¤ãƒ™ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ã¦å‡¦ç†çŠ¶æ³ã‚’æ›´æ–°
                        # ãƒãƒ¼ãƒ‰åã‚’å–å¾—ã—ã¦çŠ¶æ…‹ã‚’æ›´æ–°
                        node_name = chunk['last_node']
                        if node_name == "query_analyzer":
                            status.update(label="è³ªå•ã®åˆ†æä¸­...", state="running")
                        elif node_name == "search":
                            status.update(label="é–¢é€£æƒ…å ±ã®æ¤œç´¢ä¸­...", state="running")
                        elif node_name == "information_evaluator":
                            status.update(label="æƒ…å ±ã®è©•ä¾¡ä¸­...", state="running")
                        elif node_name == "information_completer":
                            status.update(label="æƒ…å ±ã®è£œå®Œä¸­...", state="running")
                        elif node_name == "response_generator":
                            status.update(label="å›ç­”ã®ç”Ÿæˆä¸­...", state="running")
                            endflg = True
                        elif node_name == "":
                            status.update(label="å‘¼ã³å‡ºã—ä¸­...", state="running")
                        
                        # æœ€çµ‚çš„ãªå›ç­”ã‚’å–å¾—
                        if endflg and chunk.get("messages") and len(chunk["messages"]) > 0:
                            final_message = chunk["messages"][-1]
                            useful_documents = chunk['useful_documents']
                            if hasattr(final_message, "content"):
                                final_response = final_message.content
                                processing_results["final_response"] = final_response
                                status.update(label="å‡¦ç†å®Œäº†", state="complete")
                
                # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                st.subheader("å‡¦ç†è©³ç´°")
                # æ¤œç´¢çµæœã®è¡¨ç¤º
                st.write("ã€é–¢é€£æ–‡æ›¸ã€‘")
                for i, doc in enumerate(useful_documents[:3]):  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
                    st.write(f"æ–‡æ›¸ {i+1}: {doc['metadata'].get('source', 'ä¸æ˜')}")
                    with st.expander(f"å†…å®¹ã‚’è¡¨ç¤º"):
                        st.write(doc['content'])
                        st.write(f"ã‚¹ã‚³ã‚¢: {doc['score']}")
                
                # æœ€çµ‚å›ç­”
                message_placeholder.markdown(processing_results["final_response"])
                
                # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«å›ç­”ã‚’è¿½åŠ 
                st.session_state.messages.append({"role": "assistant", "content": processing_results["final_response"] if processing_results["final_response"] else "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"})
                
            except Exception as e:
                error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                message_placeholder.markdown(error_message)
                st.error(error_message)
                import traceback
                st.error(traceback.format_exc())

with tab2:
    st.header("ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡¦ç†ãƒ•ãƒ­ãƒ¼")
    st.write("RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’ç¤ºã™ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã§ã™ã€‚å„ãƒãƒ¼ãƒ‰ã¯ç•°ãªã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å½¹å‰²ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚")
    
    # ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
    try:
        # LangGraphã®ãƒ¡ãƒ¢ãƒªãƒ¼ãƒ€ã‚¤ãƒ‰ã‚’å–å¾—ã—ã¦è¡¨ç¤º
        mermaid_code = gragh_build().get_graph().draw_mermaid()
        # print(mermaid_code)  # ãƒ‡ãƒãƒƒã‚°ç”¨
        st_mermaid(mermaid_code, height=600)
    except Exception as e:
        st.error(f"ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.info("ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’è¡¨ç¤ºã™ã‚‹ã«ã¯ã€ã¾ãšã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
